{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training and inference on an example dataset",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murthylab/sleap-notebooks/blob/master/Training_and_inference_on_an_example_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlV70jDuWzea",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we'll show you how to install SLEAP on Colab, download a dataset from the repository of sample datasets, run training and inference on that dataset using the SLEAP command-line interface, and then download the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX9noEb8m8re",
        "colab_type": "text"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Before we install SLEAP we need to set Colab to use TensorFlow 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hJvyI24KXlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5WL10cao-sc",
        "colab_type": "text"
      },
      "source": [
        "Let's confirm that we have a GPU available. This next line should return something like \"/device:GPU:0\". If you instead see an empty string as the result, go to \"Notebook settings\" in the \"Edit\" menu and select \"GPU\" as the hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I12Lk6TKZnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb8e8af9-7f16-4478-8e5b-e80b6769cba7"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMjYpcVFqWgV"
      },
      "source": [
        "Since Colab already has TensorFlow and GPU drivers installed we can install SLEAP using `pip`.\n",
        "\n",
        "Note: This installation method should also work on other Linux machines, such as an HPC cluster, or on any system where you aren't planning to use a GPU. To use a GPU on a Windows machine you'll need to install using `conda`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUfnkxMtLcK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6340ef1-eaac-42ef-f8d4-bcc499feb57b"
      },
      "source": [
        "!pip install sleap==1.1.0a10"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sleap==1.1.0a10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/52/e80875c50a649da56377c7884d5b937260b2310f8c2543dca9bf6c3c06d4/sleap-1.1.0a10-py3-none-any.whl (39.5MB)\n",
            "\u001b[K     |████████████████████████████████| 39.5MB 89kB/s \n",
            "\u001b[?25hCollecting cattrs==1.0.0rc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/f6/30e2b78920dae271ba94cb74a808021d88840b156c424c029f6255b72d3e/cattrs-1.0.0rc0-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (3.13)\n",
            "Collecting opencv-python-headless==4.2.0.34\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2c/909a04b07360516953beaf6f66480bb6b84b817c6b300c1235bfb2901ad8/opencv_python_headless-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 114kB/s \n",
            "\u001b[?25hCollecting imgstore==0.2.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/bb/a1099d1564c1f85752251a9549037ce36021f2b318c896be2f2b067d7e9a/imgstore-0.2.9-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 47.9MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ad/769c195c72ac72040635c66cd9ba7b0f4b4fc1ac67e59b99fa6988446c22/tensorflow-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 49kB/s \n",
            "\u001b[?25hCollecting jsonpickle==1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (0.22.2.post1)\n",
            "Collecting imgaug==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/df/5a3bba95b4600d5ca7aff072082ef0d9837056dd28cc4e738e7ce88dd8f8/imgaug-0.3.0-py2.py3-none-any.whl (819kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (5.4.8)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (0.11.1)\n",
            "Collecting attrs==19.3\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
            "Collecting python-rapidjson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/8f/50488049033aba6943659d7906a45947ea5b510b74e7d44ab9d7457e1710/python_rapidjson-1.0-cp36-cp36m-manylinux2010_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 37.7MB/s \n",
            "\u001b[?25hCollecting pykalman==0.9.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/62/a4adc4516bd5974aa5583090199dd4b78d1e87018d14e9279f72ccbf0b9b/pykalman-0.9.5.tar.gz (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 60.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (2.10.0)\n",
            "Collecting rich==9.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/60/765dc36f3623c22fd08f152bfae65cc9f0d815c05503ea50f7c189486d10/rich-9.10.0-py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (1.1.5)\n",
            "Collecting scikit-video\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/c69cad508139a342810ae46e946ebb3256aa6e42f690d901bb68f50582e3/scikit_video-1.1.11-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 51.9MB/s \n",
            "\u001b[?25hCollecting segmentation-models==1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/da/b9/4a183518c21689a56b834eaaa45cad242d9ec09a4360b5b10139f23c63f4/segmentation_models-1.0.1-py3-none-any.whl\n",
            "Collecting qimage2ndarray==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/16/fdd8930ef4f5795afc3cf89f056645ff012da828a4ddcc6a3d6d41edfb53/qimage2ndarray-1.8.tar.gz (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 61.3MB/s \n",
            "\u001b[?25hCollecting jsmin\n",
            "  Downloading https://files.pythonhosted.org/packages/17/73/615d1267a82ed26cd7c124108c3c61169d8e40c36d393883eaee3a561852/jsmin-2.2.2.tar.gz\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (0.16.2)\n",
            "Collecting numpy<1.19.0,>=1.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from sleap==1.1.0a10) (22.0.2)\n",
            "Collecting PySide2==5.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/ea/4699f9a83069751b98b038733b3123f5774da939ebba29a601964280a756/PySide2-5.14.1-5.14.1-cp35.cp36.cp37.cp38-abi3-manylinux1_x86_64.whl (165.5MB)\n",
            "\u001b[K     |████████████████████████████████| 165.5MB 90kB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->sleap==1.1.0a10) (4.4.2)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1->sleap==1.1.0a10) (1.6.3)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/92/dfd892312d822f36c55366118b95d914e5f16de11044a27cf10a7d71bbbf/commonmark-0.9.1-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from rich==9.10.0->sleap==1.1.0a10) (2.6.1)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting efficientnet==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
            "Collecting image-classifiers==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/81/98/6f84720e299a4942ab80df5f76ab97b7828b24d1de5e9b2cbbe6073228b7/image_classifiers-1.0.0-py3-none-any.whl\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->sleap==1.1.0a10) (1.1.1)\n",
            "Collecting shiboken2==5.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/9b/afca43974697e7b50ec22d3ad0c878a3d07eb89bfe75d341707ff2421772/shiboken2-5.14.1-5.14.1-cp35.cp36.cp37.cp38-abi3-manylinux1_x86_64.whl (847kB)\n",
            "\u001b[K     |████████████████████████████████| 849kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->sleap==1.1.0a10) (3.3.3)\n",
            "Building wheels for collected packages: pykalman, qimage2ndarray, jsmin\n",
            "  Building wheel for pykalman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykalman: filename=pykalman-0.9.5-cp36-none-any.whl size=48464 sha256=ece7fb3031d731c88c25105e6769bf8286c265ab0b21ddb3e6a9767f846d66b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/e8/6a/553d9832679cb74a8434fa597c3abdb07313e40054a0adf9ac\n",
            "  Building wheel for qimage2ndarray (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qimage2ndarray: filename=qimage2ndarray-1.8-cp36-none-any.whl size=11696 sha256=66c606eba31999f579909499647675ba070a13a4a059f87531ae3942a9017878\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/ef/1d/c77ef6d3d3fe72aa88f822a05e29b17e79914af69ddeff4c02\n",
            "  Building wheel for jsmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsmin: filename=jsmin-2.2.2-cp36-none-any.whl size=13921 sha256=cc49a8d5f7f82e8281f13e3df8563138c318d73f2c2aba723739ccd9391dcc80\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/f4/de/9667d84f759289edf5442220997c6d4334637a6bb2a7b90f73\n",
            "Successfully built pykalman qimage2ndarray jsmin\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: attrs, cattrs, numpy, opencv-python-headless, imgstore, tensorflow-estimator, tensorflow, jsonpickle, imgaug, python-rapidjson, pykalman, commonmark, colorama, rich, scikit-video, keras-applications, efficientnet, image-classifiers, segmentation-models, qimage2ndarray, jsmin, shiboken2, PySide2, sleap\n",
            "  Found existing installation: attrs 20.3.0\n",
            "    Uninstalling attrs-20.3.0:\n",
            "      Successfully uninstalled attrs-20.3.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed PySide2-5.14.1 attrs-19.3.0 cattrs-1.0.0rc0 colorama-0.4.4 commonmark-0.9.1 efficientnet-1.0.0 image-classifiers-1.0.0 imgaug-0.3.0 imgstore-0.2.9 jsmin-2.2.2 jsonpickle-1.2 keras-applications-1.0.8 numpy-1.18.5 opencv-python-headless-4.2.0.34 pykalman-0.9.5 python-rapidjson-1.0 qimage2ndarray-1.8 rich-9.10.0 scikit-video-1.1.11 segmentation-models-1.0.1 shiboken2-5.14.1 sleap-1.1.0a10 tensorflow-2.3.1 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq7jrgUksLtR"
      },
      "source": [
        "## Getting your training data into Colab\n",
        "\n",
        "You'll need to get your training data into Colab. The easiest way to do this is to export a self-contained **training package** from SLEAP and copy that onto [Google Drive](https://www.google.com/drive).\n",
        "\n",
        "For this demo we'll instead download a sample dataset from the SLEAP [sample datasets repository](https://github.com/murthylab/sleap-datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm3cU1Bc0tWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ac5677-e3c5-477c-a2f7-44d619208b22"
      },
      "source": [
        "!apt-get install tree\n",
        "!wget -O dataset.zip https://github.com/murthylab/sleap-datasets/releases/download/dm-courtship-v1/drosophila-melanogaster-courtship.zip\n",
        "!mkdir dataset\n",
        "!unzip dataset.zip -d dataset\n",
        "!rm dataset.zip\n",
        "!tree dataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (150 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 146425 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "--2021-02-13 17:53:50--  https://github.com/murthylab/sleap-datasets/releases/download/dm-courtship-v1/drosophila-melanogaster-courtship.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/263375180/16df8d00-94f1-11ea-98d1-6c03a2f89e1c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210213T175350Z&X-Amz-Expires=300&X-Amz-Signature=a5a933e7a64c8dafa46f54a14795f60f1110b578d2db876a19bce92934ad3240&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=263375180&response-content-disposition=attachment%3B%20filename%3Ddrosophila-melanogaster-courtship.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-02-13 17:53:50--  https://github-releases.githubusercontent.com/263375180/16df8d00-94f1-11ea-98d1-6c03a2f89e1c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210213T175350Z&X-Amz-Expires=300&X-Amz-Signature=a5a933e7a64c8dafa46f54a14795f60f1110b578d2db876a19bce92934ad3240&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=263375180&response-content-disposition=attachment%3B%20filename%3Ddrosophila-melanogaster-courtship.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111973079 (107M) [application/octet-stream]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>] 106.79M  52.2MB/s    in 2.0s    \n",
            "\n",
            "2021-02-13 17:53:52 (52.2 MB/s) - ‘dataset.zip’ saved [111973079/111973079]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/drosophila-melanogaster-courtship/\n",
            "  inflating: dataset/drosophila-melanogaster-courtship/.DS_Store  \n",
            "   creating: dataset/__MACOSX/\n",
            "   creating: dataset/__MACOSX/drosophila-melanogaster-courtship/\n",
            "  inflating: dataset/__MACOSX/drosophila-melanogaster-courtship/._.DS_Store  \n",
            "  inflating: dataset/drosophila-melanogaster-courtship/20190128_113421.mp4  \n",
            "  inflating: dataset/__MACOSX/drosophila-melanogaster-courtship/._20190128_113421.mp4  \n",
            "  inflating: dataset/drosophila-melanogaster-courtship/courtship_labels.slp  \n",
            "  inflating: dataset/__MACOSX/drosophila-melanogaster-courtship/._courtship_labels.slp  \n",
            "  inflating: dataset/drosophila-melanogaster-courtship/example.jpg  \n",
            "  inflating: dataset/__MACOSX/drosophila-melanogaster-courtship/._example.jpg  \n",
            "dataset\n",
            "├── drosophila-melanogaster-courtship\n",
            "│   ├── 20190128_113421.mp4\n",
            "│   ├── courtship_labels.slp\n",
            "│   └── example.jpg\n",
            "└── __MACOSX\n",
            "    └── drosophila-melanogaster-courtship\n",
            "\n",
            "3 directories, 3 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ-sr67av5uu"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now you're ready to train a model! We'll use the command-line interface for training, and train a model for confidence maps using the default **training profile**. The training profile determines the model architecture, the learning rate for training, and other training parameters.\n",
        "\n",
        "When you start running this cell, you'll see the training parameters listed and then you'll see the training and validation loss for each training epoch.\n",
        "\n",
        "If you're happy with the validation loss you see for an epoch during training, you're welcome to stop training by clicking the stop button next to the notebook cell running training. The version of the model with the lowest validation loss is saved during training, and that's what will be used for inference. If you don't stop training, it will run for 200 epochs or until validation loss fails to improve for some number of epochs (controlled by the `early_stopping` fields in the training profile)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKf6qzMqNBUi"
      },
      "source": [
        "!sleap-train baseline.centroid.json \"dataset/drosophila-melanogaster-courtship/courtship_labels.slp\" --run_name \"courtship.centroid\" --video-paths \"dataset/drosophila-melanogaster-courtship/20190128_113421.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm3i0ry04IMx"
      },
      "source": [
        "Let's also train a centered instance model since we'll need both a centroid model and a top-down confidence maps model to run inference with the top-down pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufbULTDw4Hbh"
      },
      "source": [
        "!sleap-train baseline_medium_rf.topdown.json \"dataset/drosophila-melanogaster-courtship/courtship_labels.slp\" --run_name \"courtship.topdown_confmaps\" --video-paths \"dataset/drosophila-melanogaster-courtship/20190128_113421.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whOf8PaFxYbt"
      },
      "source": [
        "The models (along with the profiles and ground truth data used to train and validate the model) are saved in the `models/` directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBUTQ2Cm44En",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ca298981-af65-43b3-f0f6-573f423acba8"
      },
      "source": [
        "!tree models/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "models/\n",
            "├── courtship.centroid\n",
            "│   ├── best_model.h5\n",
            "│   ├── initial_config.json\n",
            "│   ├── labels_gt.train.slp\n",
            "│   ├── labels_gt.val.slp\n",
            "│   ├── training_config.json\n",
            "│   └── training_log.csv\n",
            "└── courtship.topdown_confmaps\n",
            "    ├── best_model.h5\n",
            "    ├── initial_config.json\n",
            "    ├── labels_gt.train.slp\n",
            "    ├── labels_gt.val.slp\n",
            "    ├── training_config.json\n",
            "    └── training_log.csv\n",
            "\n",
            "2 directories, 12 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIsKUX661xFK"
      },
      "source": [
        "# Inference\n",
        "\n",
        "At this point you should have SLEAP installed, the sample dataset downloaded, and trained models for centroids and topdown confidence maps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLtjtq9E1Znr"
      },
      "source": [
        "!sleap-track \"dataset/drosophila-melanogaster-courtship/20190128_113421.mp4\" --frames 0-100 -m \"models/courtship.centroid\" -m \"models/courtship.topdown_confmaps\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzObCUToEqwA"
      },
      "source": [
        "When inference is finished, it will save the predictions in a file. Since we didn't specify a path, it will be saved as `<video filename>.predictions.slp` in the same directory as the video:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6KVfWDIDEUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b0633306-f24f-4e6e-e78f-e968a765a3c6"
      },
      "source": [
        "!tree dataset/drosophila-melanogaster-courtship"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset/drosophila-melanogaster-courtship\n",
            "├── 20190128_113421.mp4\n",
            "├── 20190128_113421.mp4.predictions.slp\n",
            "├── courtship_labels.slp\n",
            "└── example.jpg\n",
            "\n",
            "0 directories, 4 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mf3KZQj_GhH"
      },
      "source": [
        "You can inspect your predictions file using `sleap-inspect`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jbVP_s06hMh"
      },
      "source": [
        "!sleap-inspect dataset/drosophila-melanogaster-courtship/20190128_113421.mp4.predictions.slp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFfHDVy7_iDz"
      },
      "source": [
        "If you're using Chrome you can download your trained models like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej2it8dl_BO_"
      },
      "source": [
        "# Zip up the models directory\n",
        "!zip -r trained_models.zip models/\n",
        "\n",
        "# Download.\n",
        "from google.colab import files\n",
        "files.download(\"/content/trained_models.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iskOQI-r_zNO"
      },
      "source": [
        "And you can likewise download your predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdXCYnRV_omC"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('dataset/drosophila-melanogaster-courtship/20190128_113421.mp4.predictions.slp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fy26NVmCWFw"
      },
      "source": [
        "In some other browsers (Safari) you might get an error and you can instead download using the \"Files\" tab in the side panel (it has a folder icon). Select \"Show table of contents\" in the \"View\" menu if you don't see the side panel."
      ]
    }
  ]
}